# -*- coding: utf-8 -*-
"""TME2_final.ipynb

Automatically generated by Colaboratory.

"""

import numpy as np
import gym
import matplotlib
import gridworld
import copy
import time

def time_it(function):
    def wrapper(*args,**kwargs):
        begin = time.time()
        result = function(*args,**kwargs)
        end = time.time()
        print(function.__name__ + " a mis : {} ms".format((end-begin)*1000))
        return result
    return wrapper

class ValueIteration(object):
    def __init__(self,env,epsilon=1e-1,gamma=0.99):
        self.env = env
        self.action_space = env.action_space
        self.states, self.P = env.getMDP()
        self.epsilon,self.gamma = epsilon,gamma
        self.V = None
        self.policy = None
        
        
    
    def get_policy(self):
        return self.policy
    
    def action(self,observation):
        return self.policy[self.env.getStateFromObs(observation)]
    
    @time_it
    def fit(self):
        
        n_state = len(self.states)
        n_action = self.action_space.n
        
        self.V = np.zeros(n_state)
        
        i = 0
        
        
        condition_V = True
        while(condition_V):
            V = self.V
            V_old = copy.deepcopy(self.V)
            
            for key_s in self.P.keys():
                somme = np.zeros(n_action)
                for action in range(n_action):
                    liste_P = self.P[key_s][action]
                    
                    for p,next_state,r,_ in liste_P : 
                        
                        somme[action] += p*(r + self.gamma*V_old[next_state])
                        
                        
                V[key_s] = max(somme)
                
                
                
            self.V = V                
            
            if(np.linalg.norm(self.V-V_old)<=self.epsilon): condition_V = False
            
            i += 1 
               
        Policy = -1*np.ones(n_state)
        for key_s in self.P.keys():
            
            somme = np.zeros(n_action)
            for action in range(n_action):
                liste_P = self.P[key_s][action]
                for p,next_state,r,_ in liste_P : 

                    somme[action] += p*(r + self.gamma*self.V[next_state])
                        
                
            Policy[key_s] = np.argmax(somme)
        
        self.policy = Policy

class PolicyIteration(object):
    def __init__(self,env,epsilon=1e-1,gamma=0.99):
        self.env = env
        self.action_space = env.action_space
        self.states, self.P = env.getMDP()
        self.epsilon,self.gamma = epsilon,gamma
        self.V = None
        self.policy = None
        
    def get_policy(self):
        
        for i in set(np.arange(len(self.states)))-set(self.P.keys()):
            self.policy[i]=-1
        
        return (self.policy)
    
    def action(self,observation):
        return self.policy[self.env.getStateFromObs(observation)]
    @time_it
    def fit(self):
        
        k = 0
        n_state = len(self.states)
        n_action = self.action_space.n
        
        self.V = np.zeros(n_state)
        self.policy = np.random.choice(np.arange(n_action),n_state)
        #-1*np.ones(n_state) #initialisation police random
        
        condition_policy = True
        while(condition_policy):
            policy = self.policy
            policy_old = copy.deepcopy(self.policy)
    
            i = 0 
            condition_V = True
            while(condition_V):
                V = self.V
                V_old = copy.deepcopy(self.V)
                for key_s in self.P.keys():
                    somme = 0
                    liste_P = self.P[key_s][self.policy[key_s]] 
                    for p,next_state,r,_ in liste_P : 
                        somme += p*(r+self.gamma*self.V[next_state])
                    
                    
                    V[key_s] = somme
                self.V = V
                #fin for s n S    
                i+= 1 
                if(np.linalg.norm(self.V-V_old)<=self.epsilon) : 
                      condition_V = False
                #fin boucle 2 
            
            for key_s in self.P.keys():
            
                somme = np.zeros(n_action)
                for action in range(n_action):
                    liste_P = self.P[key_s][action]
                    for p,next_state,r,_ in liste_P : 

                        somme[action] += p*(r + self.gamma*self.V[next_state])


                policy[key_s] = np.argmax(somme)
            k+=1
            self.policy = policy
            if((self.policy==policy_old).all()):
                    condition_policy=False

def main(agentclass,plan):

    env = gym.make("gridworld-v0")
    env.setPlan("gridworldPlans/plan"+str(plan)+".txt", {0: -0.001, 3: 1, 4: 1, 5: -1, 6: -1})

    env.seed(0)  # Initialise le seed du pseudo-random
    #print(env.action_space)  # Quelles sont les actions possibles
    #print(env.step(1))  # faire action 1 et retourne l'observation, le reward, et un done un booleen (jeu fini ou pas)
    env.render()  # permet de visualiser la grille du jeu
    env.render(mode="human") #visualisation sur la console
    states, mdp = env.getMDP()  # recupere le mdp et la liste d'etats
    #print("Nombre d'etats : ",len(states))
    state, transitions = list(mdp.items())[0]
    #print(state)  # un etat du mdp
    #print(transitions)  # dictionnaire des transitions pour l'etat :  {action-> [proba,etat,reward,done]}

    # Execution avec un Agent
    agent = agentclass(env)
    agent.fit()


    episode_count = 1000
    reward = 0
    done = False
    rsum = 0

    for i in range(1):
        obs = env.reset()
        env.verbose = (i % 100 == 0 and i > 0)  # afficher 1 episode sur 100
        if env.verbose:
            env.render()
        j = 0
        rsum = 0
        while True:
            action = agent.action(obs)
            obs, reward, done, _ = env.step(action)
            rsum += reward
            j += 1
            if env.verbose:
                env.render()
            if done:
                print("Episode : " + str(i) + " rsum=" + str(rsum) + ", " + str(j) + " actions")
                break

    print("done")
    env.close()

#Policy Iteration : fit a mis 4.001617431640625 ms
#Value Iteration : fit a mis : 3.000497817993164 ms

#Faire des expériences sur les autres cartes et voir les problèmes que ca peut causer



plans = range(0,11)
agentclasses = [PolicyIteration,ValueIteration]

for agentclass in agentclasses:
    for plan in plans:
        if(plan!=9 and plan!=8):
            print("Map : "+str(plan) + " | Algorithme : "+str(agentclass))
            main(agentclass,plan)

for agentclass in agentclasses:
    print("Map : "+str(8) + " | Algorithme : "+str(agentclass))
    main(agentclass,8)

# MAP 9 : RecursionError: maximum recursion depth exceeded while calling a Python object

main(0,9)

