# -*- coding: utf-8 -*-
"""DDPG.ipynb

Automatically generated by Colaboratory.

"""


# Commented out IPython magic to ensure Python compatibility.
import numpy as np 
import scipy
import torch 
import torch.nn as nn
from torch.utils.tensorboard import SummaryWriter
import copy
import time
from core import *
from memory import *
from utils import *
from torch.distributions import Categorical
from ac import *
# %load_ext tensorboard

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') #A UTILISER PLUS TARD

def make_config(configName,env,nbEpisodes=100,startSteps=8000,maxLengthTrain=500,maxLengthTest=500,nbTest=1,freqUpdate=50,freqTest=10,freqVerbose=10):

  text = ["env: "+env   ,                                            
          "seed: 5",
          "featExtractor: !!python/name:__main__.NothingToDo ''",
          "nbEpisodes: "+str(nbEpisodes),
          "maxLengthTrain : "+str(maxLengthTrain),
          "maxLengthTest : "+str(maxLengthTest),
          "nbTest : "+str(nbTest),
          "freqUpdate : "+str(freqUpdate),
          "freqTest : "+str(freqTest),
          "freqVerbose : "+str(freqVerbose),
          "startSteps : "+str(startSteps),
          ]
  with open(configName,'w') as f:
    
    f.write('\n'.join(text))
  return configName



class DDPGAgent(nn.Module):
  def __init__(self,env,opt,logger): 
    super(DDPGAgent,self).__init__()
    self.opt = opt
    self.env = env
    self.logger = logger
    self.test = False
    
    self.featureExtractor = self.opt['featExtractor'](self.env)

    self.n_actions = self.env.action_space.shape[0]
    self.obs_size = self.featureExtractor.outSize

    self.low = torch.from_numpy(self.env.action_space.low).to(torch.float32).to(device)
    self.high = torch.from_numpy(self.env.action_space.high).to(torch.float32).to(device)
    self.bruit = Orn_Uhlen(self.n_actions)

    self.ac = ActorCritic(self.obs_size, self.n_actions,self.low,self.high).to(device).to(torch.float32)
    self.ac_target = copy.deepcopy(self.ac)
    
    for p in self.ac_target.parameters(): #on freeze tout les parametres du target qu'on mettera à jour manuellement
        p.requires_grad = False

    #Hyperparametres 
    self.rho = 0.995
    self.gamma = 0.99
    self.actor_lr, self.critic_lr = 0.001, 0.001
    self.batch_size = 100

    self.buffer = Memory(int(1e6))
    
    #BACKWARD 

    self.actor_optimizer = torch.optim.Adam(self.ac.actor.parameters(), lr = self.actor_lr)
    self.critic_optimizer = torch.optim.Adam(self.ac.critic.parameters(), lr = self.critic_lr)

    self.critic_criterion = nn.SmoothL1Loss().to(device)
    



  def act(self,obs):
    bruit = self.bruit.sample().to(device)
    action = self.ac.act(obs)
    return torch.clamp(action+bruit,min=self.low,max=self.high).detach().cpu().numpy()

  def loss_critic(self,obs, actions, rewards, next_obs, dones): 
    Q = self.ac.critic(obs,actions)

    with torch.no_grad():
      Q_target = self.ac_target.critic(next_obs,self.ac.actor(next_obs))
      target = rewards + self.gamma * (1-dones) * Q_target.unsqueeze(1)

      
   
    loss = self.critic_criterion(target.squeeze(1),Q).mean()
    return loss

  def loss_actor(self,obs):
    Q = self.ac.critic(obs,self.ac.actor(obs))
    loss = -Q.mean()
    return loss

  def learn(self):
    if not self.test:
      obs, actions, rewards, next_obs, dones = self.sample()
      
      #UPDATE DU CRITIC
      self.critic_optimizer.zero_grad()
      loss_critic = self.loss_critic(obs, actions, rewards, next_obs, dones)
      loss_critic.backward()
      self.critic_optimizer.step()
      
      for p in self.ac.critic.parameters(): #On freeze les parametres du critic car on l'a deja optimiser
              p.requires_grad = False
      #UPDATE DE ACTOR    
      self.actor_optimizer.zero_grad()
      loss_actor = self.loss_actor(obs)
      loss_actor.backward()
      self.actor_optimizer.step()

      for p in self.ac.critic.parameters(): #on defreeze 
              p.requires_grad = True
        
      self.update_targets()
    else:
      pass

  def store(self,ob, action, reward, next_ob, done):
    self.buffer.store((ob, action, reward, next_ob, done))

  def sample(self):
    sampled_trs = self.buffer.sample(self.batch_size)[2]
    obs, actions, rewards, next_obs, dones = [x for x in zip(*sampled_trs)]
    return (torch.vstack(obs).to(device), 
            torch.vstack(actions).to(device), 
            torch.tensor(rewards,device=device,dtype=torch.float32).reshape(len(rewards),1),
            torch.vstack(next_obs).to(device), 
            torch.tensor(dones,dtype=torch.float32,device=device).reshape(len(dones),1))


  @torch.no_grad()
  def update_targets(self):
    for p_ac, p_ac_target in zip(self.ac.parameters(),self.ac_target.parameters()):
      p_ac_target.data.mul_(self.rho) #on a fait les parametre target p_target <- rho*p_target 
      p_ac_target.data.add_((1-self.rho)*p_ac.data)

def main(config_file,name,agentClass):
  env, config, outdir, logger = init(config_file,name)

  freqTest = config["freqTest"]
  freqUpdate = config["freqUpdate"] 
  nbTest = config["nbTest"]
  env.seed(config["seed"])
  np.random.seed(config["seed"])
  episode_count = config["nbEpisodes"]
  maxLengthTrain = config["maxLengthTrain"]
  maxLengthTest = config["maxLengthTest"]
  startSteps = config["startSteps"]


  agent = agentClass(env,config,logger)

  rsum = 0 #somme cumulé pour chaque épisode 
  mean = 0
  verbose = True
  itest = 0
  i=0
  reward = 0
  done = False
  condition_update = False


  for episode in range(episode_count):
    checkConfUpdate(outdir,config)

    rsum = 0 #on remet à 0 la somme cumulé à chaque episode
    ob = env.reset() #premiere observation, on commence le jeu 

    if episode % int(config["freqVerbose"]) == 0: #on affiche le jeu tout les 10 épisodes
      verbose = True
    else:
      verbose = False

    if episode%freqTest==0 and episode>=freqTest: #On test l'agent tout les 10 épisodes (2condition car episode=0 au debut)
      print("Test time!\n")
      mean = 0 #moyenne des rsum en phase de test pour voir comment notre agent se débrouille en moyenne
      agent.test = True #on dit à l'agent de passer en test, donc on ne stock plus les transitions et on ne s'entraine plus 

    if episode % freqTest == nbTest and episode > freqTest: #Fin du test
            print("End of test, mean reward=", mean / nbTest)
            itest += 1
            logger.direct_write("rewardTest", mean / nbTest, itest)
            agent.test = False
    

    
    j = 0 #nb de transition
    if verbose:
      #env.render()
      pass
    
    new_ob = agent.featureExtractor.getFeatures(ob) #on extrait les features de l'observation initial
    new_ob = torch.from_numpy(new_ob).to(device).to(torch.float32)
    while(True):
      if verbose: #on affiche à chaque étape le jeu
        #env.render()
        pass
        
      ob = new_ob

      if(i>=startSteps):
        action = agent.act(ob)
      else:
        action = env.action_space.sample()

      new_ob, reward, done, _ = env.step(action)

      new_ob = agent.featureExtractor.getFeatures(new_ob)
      new_ob = torch.from_numpy(new_ob).to(device).to(torch.float32)

      j+=1
      i+=1

      #on regarde si on a atteint la taille max de transitions 1er condition en phase de train, 2eme en phase de test 
      if ((maxLengthTrain > 0) and (not agent.test) and (j == maxLengthTrain)) or ( (agent.test) and (maxLengthTest > 0) and (j == maxLengthTest)):
                done = True #on force la fin de l'episode même si on n'a pas atteint un état final 
                print("forced done!")

      agent.store(ob, torch.from_numpy(action), reward, new_ob, done) #on sauvegarde nos transitions uniquement en phase de train , on ajoute le log_probs 
      rsum += reward

      
      
      if i > startSteps and i % freqUpdate == 0:
        for _ in range(freqUpdate):
            agent.learn()
    
      
      
      if(done): #si on atteint un etat final ou qu'on met fin a l'épisode
        print(str(episode) + " rsum=" + str(rsum) + ", " + str(j) + " actions ")
        logger.direct_write("reward", rsum, episode)
        mean += rsum #on sauvegarde le reward cumulé dans une variable qui nous servira à calculer la moyenne
        rsum = 0 #on remet le reward cumulé à 0 pour reprendre un nouveau épisode
        
        
        break
  
  env.close()
  return outdir

def run_env(nbEpisodes=1000):
  outdirs = []
  list_class = [DDPGAgent]
  list_name = ["DDPGAgent"]
  env_name = ['Pendulum-v0',
              "MountainCarContinuous-v0",
              #"LunarLanderContinuous-v2"
              ] #
  for agent_class,agent_name in zip(list_class,list_name):
    
    for env in env_name:
      print("######CHANGEMENT######\n")
      print(agent_name+"_"+env)
      file_config = make_config(agent_name+"_"+env+".yaml",env,nbEpisodes)
      outdir = main(file_config,agent_name+"_"+env,agent_class) 
      outdirs.append(outdir)
  return outdirs

if __name__ == "__main__": #On va mtn faire en sorte que notre algo s'entraine 
    outdirs = run_env(100)

    outdirs

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir=./XP/Pendulum-v0/DDPGAgent_Pendulum-v0_19-12-2021-15H18-20S

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir=./XP/MountainCarContinuous-v0/DDPGAgent_MountainCarContinuous-v0_19-12-2021-15H20-55S

