# -*- coding: utf-8 -*-
"""DQN.ipynb

Automatically generated by Colaboratory.

"""



import torch 
import torch.nn as nn
from torch.utils.tensorboard import SummaryWriter
import copy
from core import *
from memory import *
from utils import *
# %load_ext tensorboard

import gridworld

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

def make_config_gridworld(configName, map = 1,nbEpisodes=10000):

  text = ["env: gridworld-v0",
          "map: gridworldPlans/plan"+str(map)+".txt",
          "rewards : {0: -0.001, 3: 1, 4: 1, 5: -1, 6: -1}",
          "seed: 5",                                 
          "freqSave: 1000"  ,                                                                      # frequence de sauvegarde du modèle
          "freqTest: 100 " ,                                                                        # frequence de test
          "nbTest: 1     "  ,                                                                      # nb de tests à effectuer tous les freqTest trajectoires
          "freqVerbose: 1000",                                                                       # frequence d'affichage de l'environnement
          "freqOptim: 1",
          "fromFile: null",
          "nbEpisodes: "+str(nbEpisodes),
          "maxLengthTest: 500"  ,                                                                  # Longueur maxi des trajectoires en Test
          "maxLengthTrain: 100"  ,                                                                 # Longueur maxi des trajectoires en Train
          "featExtractor: !!python/name:__main__.MapFromDumpExtractor2 ''  ",                                                                       
          "execute: | ",
          ' env.setPlan(config["map"], config["rewards"])']
  with open(configName,'w') as f:
    
    f.write('\n'.join(text))
  return configName

def make_config(configName, env,nbEpisodes=15000,usePer=True,useTarget=True): #CartPole-v1 ou LunarLander-v2

  text = ["env: "+env   ,                                               # environnement
          "seed: 5",
          "featExtractor: !!python/name:__main__.NothingToDo ''",
          "freqSave: 1000",
          "freqTest: 10",
          "nbTest: 1",
          "freqVerbose: 10",
          "freqOptim: 1   "   ,                                                                    # Optimisation de la politique tous les freqOptim evenements
          "fromFile: null",
          "nbEpisodes: "+str(nbEpisodes),
          "usePer: "+str(usePer),
          "useTarget: "+str(useTarget),
          "maxLengthTrain : 500",
          "maxLengthTest : 500"]
  with open(configName,'w') as f:
    
    f.write('\n'.join(text))
  return configName


class DQNAgent(nn.Module):

  def __init__(self,env,opt): #env : environnement, opt : config qui répertorie nos paramètres
    super(DQNAgent,self).__init__()
    self.opt = opt
    self.env = env
    if self.opt['fromFile'] is not None : 
      self.load(self.opt['fromFile']) #on charge d'anciens paramètres pour notre modèle
    self.featureExtractor = self.opt['featExtractor'](self.env) #On récupere la fonction features extractor phi
    
    #hyperparameters 
    self.epsilon = 0.1
    self.discount = 0.99
    self.lr = 0.0001

    #Config de l'environnement
    self.action_space = self.env.action_space
    self.n_actions = self.action_space.n #Nombre d'actions
    self.observation_space = self.env.observation_space


    self.usePer = self.opt.usePer
    self.useTarget = self.opt.useTarget
    
    
    self.observations_shape = self.featureExtractor.outSize

    #Modele
    self.Q_network = NN(self.observations_shape,self.n_actions,[200]).to(torch.float32).to(device)
    self.Q_target = copy.deepcopy(self.Q_network)

    self.criterion = nn.SmoothL1Loss().to(device)
    self.optimizer = torch.optim.Adam(self.Q_network.parameters(),lr=self.lr)

    #Memoire 
    self.N = 10000 #capacité de la mémoire du buffer qui va stocker les transitions
    self.batch_size = 128
    self.buffer = Memory(self.N,self.usePer)
    self.test = False 
    #Nombre de transitions totale en prenant tout les episodes (meme les test le font augmenter)
    self.nbEvents = 0 
    
  
  def act_target(self,obs):
    return torch.argmax(self.Q_target(obs),1) 

  @torch.no_grad()
  def act(self,obs):
    if(torch.rand(1)>self.epsilon):
      self.epsilon*=0.999
      return torch.argmax(self.Q_network(obs),1)
    else:
      self.epsilon*=0.999
      return torch.tensor(self.action_space.sample(),device=device)
    
  def sample(self):
    sampled_trs = self.buffer.sample(self.batch_size)[2]
    obs, actions, rewards, new_obs, dones = [x for x in zip(*sampled_trs)]
    return (torch.vstack(obs).to(device), 
            torch.vstack(actions).to(device), 
            torch.tensor(rewards,device=device,dtype=torch.float32).reshape(len(rewards),1),
            torch.vstack(new_obs).to(device), 
            torch.tensor(dones,dtype=torch.float32,device=device).reshape(len(dones),1))
  
  
  
  def learn(self): #on va entrainer notre réseau de neurone 
    if not self.test: #on vérifie bien qu'on est pas en phase de test encore une fois
      obs, actions, rewards, new_obs, dones = self.sample()

      with torch.no_grad():
       
        if(self.useTarget):
          next_q_values_target = self.Q_target(new_obs)
        else:
          next_q_values_target = self.Q_network(new_obs)
        next_state_max_q_values = torch.amax(next_q_values_target,dim=1).unsqueeze(1)
        
        
        returns = rewards + (1-dones)*self.discount*next_state_max_q_values



      
      q_values = self.Q_network(obs)
      state_action_values = q_values.gather(1, actions)
      
      

      self.optimizer.zero_grad()
      loss = self.criterion(state_action_values,returns.detach()).mean()
      loss.backward()
      self.optimizer.step()
      


    pass

  def save(self):
    #torch.save(self.Q_network.state_dict(),'./content')
    pass
  def load(self):
    pass


  def store(self,ob, action, new_ob, reward, done, it):
    # Si l'agent est en mode de test, on n'enregistre pas la transition
    if not self.test:
        # si on atteint la taille max d'episode en apprentissage, alors done ne devrait pas etre a true (episode pas vraiment fini dans l'environnement)
        if it == self.opt.maxLengthTrain:
          print("undone")
          done=False
        tr = (ob, action, reward, new_ob, done)
        self.buffer.store(tr)


  def isTimeToLearn(self):

    if(self.test): #si on test alors on ne s'entraine pas 
      return False 
    self.nbEvents += 1
    #On s'entraine toute les freqOptim sur une trajectoire
    #condition = (self.nbEvents%self.opt['freqOptim'] == 0) 
    condition = (self.nbEvents%self.opt['freqOptim'] == 0) #on met a jour tout les 10 trans
    return condition 

  def update(self):
    self.Q_target.load_state_dict(self.Q_network.state_dict())

outdirs_safe = []

def main(config_file,name,agentClass):
  env, config, outdir, logger = init(config_file,name)
  outdirs_safe.append(outdir)

  freqTest = config["freqTest"]
  freqSave = config["freqSave"]
  freqUpdate = 1000 #On met a jour le target network tout les 100 transitions
  nbTest = config["nbTest"]
  env.seed(config["seed"])
  np.random.seed(config["seed"])
  episode_count = config["nbEpisodes"]
  
  maxLengthTrain = 500
  maxLengthTest = 500

  agent = agentClass(env,config)

  rsum = 0 #somme cumulé pour chaque épisode 
  mean = 0
  verbose = True
  itest = 0
  reward = 0
  done = False

  for episode in range(episode_count):
    checkConfUpdate(outdir,config)

    rsum = 0 #on remet à 0 la somme cumulé à chaque episode
    ob = env.reset() #premiere observation, on commence le jeu 

    if episode % int(config["freqVerbose"]) == 0: #on affiche le jeu tout les 10 épisodes
      verbose = True
    else:
      verbose = False

    if episode%freqTest==0 and episode>=freqTest: #On test l'agent tout les 10 épisodes (2condition car episode=0 au debut)
      print("Test time!\n")
      mean = 0 #moyenne des rsum en phase de test pour voir comment notre agent se débrouille en moyenne
      agent.test = True #on dit à l'agent de passer en test, donc on ne stock plus les transitions et on ne s'entraine plus 

    if episode % freqTest == nbTest and episode > freqTest: #Fin du test
            print("End of test, mean reward=", mean / nbTest)
            itest += 1
            logger.direct_write("rewardTest", mean / nbTest, itest)
            agent.test = False
    
    if(episode%freqSave==0):
      agent.save() #a faire

    
    j = 0 #nb de transition
    if verbose:
      #env.render()
      pass
    
    new_ob = agent.featureExtractor.getFeatures(ob) #on extrait les features de l'observation initial
    new_ob = torch.from_numpy(new_ob).to(device).to(torch.float32)
    while(True):
      if verbose: #on affiche à chaque étape le jeu
        #env.render()
        pass

      ob = new_ob
      
      
      action = agent.act(ob)
      new_ob, reward, done, _ = env.step(action.item())
      new_ob = agent.featureExtractor.getFeatures(new_ob) #on recupere un array selon feature
      new_ob = torch.from_numpy(new_ob).to(device).to(torch.float32) #on reconvertit tout ca en tenseur 

      j+=1

      #on regarde si on a atteint la taille max de transitions 1er condition en phase de train, 2eme en phase de test 
      if ((maxLengthTrain > 0) and (not agent.test) and (j == maxLengthTrain)) or ( (agent.test) and (maxLengthTest > 0) and (j == maxLengthTest)):
                done = True #on force la fin de l'episode même si on n'a pas atteint un état final 
                print("forced done!")

      agent.store(ob, action, new_ob, reward, done,j) #on sauvegarde nos transitions uniquement en phase de train 
      rsum += reward

      if agent.nbEvents%freqUpdate == 0 :
        agent.update()

      if agent.isTimeToLearn():
        agent.learn()
      
      
      
      if(done): #si on atteint un etat final ou qu'on met fin a l'épisode
        print(str(episode) + " rsum=" + str(rsum) + ", " + str(j) + " actions ")
        logger.direct_write("reward", rsum, episode)
        mean += rsum #on sauvegarde le reward cumulé dans une variable qui nous servira à calculer la moyenne
        rsum = 0 #on remet le reward cumulé à 0 pour reprendre un nouveau épisode

        break
  
  env.close()
  return outdir

if __name__ == "__main__": #On va mtn faire en sorte que notre algo s'entraine 
  outdirs = []
  list_class = [DQNAgent]
  list_name = ["DQNAgent"]
  env_name = ["CartPole-v1",
              #"LunarLander-v2"
              ] #

  PERs = [False,True]
  Targs = [False,True]
  
  PERs_name = ["nP","P"]
  Targs_name = ["nT","T"]
  #for map in maps:
  for agent_class,agent_name in zip(list_class,list_name):
    for env in env_name:
      for per,per_name in zip(PERs,PERs_name):
        for targ,targ_name in zip(Targs,Targs_name):
          if(targ==True and per ==True):
            break
          print("######CHANGEMENT######\n")
          print(agent_name+"_"+env)
          file_config = make_config(agent_name+"_"+env+".yaml",env,2000,per,targ)
          outdir = main(file_config,agent_name+"_"+env+"_"+per_name+"_"+targ_name,agent_class)
          outdirs.append(outdir)

"""######CHANGEMENT######

DQNAgent_CartPole-v1
Saving in ./XP/CartPole-v1/DQNAgent_CartPole-v1_19-12-2021-17H52-54S
tensorboard --logdir=./XP/CartPole-v1/DQNAgent_CartPole-v1_19-12-2021-17H52-54S"""



# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir=./XP/CartPole-v1/DQNAgent_CartPole-v1_nP_nT_20-12-2021-12H11-57S

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir=./XP/CartPole-v1/DQNAgent_CartPole-v1_P_nT_20-12-2021-12H15-05S

if __name__ == "__main__": #On va mtn faire en sorte que notre algo s'entraine 
  outdirs = []
  list_class = [DQNAgent]
  list_name = ["DQNAgent"]
  maps = [5]

  for map in maps:
    for agent_class,agent_name in zip(list_class,list_name):
      #for env in env_name:
        print("######CHANGEMENT######")
        print(agent_name+"_"+str(map))
        file_config = make_config_gridworld(agent_name+"_"+str(map)+".yaml",map,100)
        outdir = main(file_config,agent_name+"_"+str(map),agent_class)
        outdirs.append(outdir)


# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir=./XP/gridworld-v0/DQNAgent_5_19-12-2021-21H31-44S

