# -*- coding: utf-8 -*-
"""SAC.ipynb

Automatically generated by Colaboratory.

"""


# Commented out IPython magic to ensure Python compatibility.
import numpy as np 
import scipy.signal
import torch 
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.tensorboard import SummaryWriter
import copy
import time
from core import *
from memory import *
from utils import *
from torch.distributions.normal import Normal
import itertools
from ac import *
# %load_ext tensorboard

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') #A UTILISER PLUS TARD

def make_config(configName,env,nbEpisodes=100,startSteps=8000,maxLengthTrain=500,maxLengthTest=500,nbTest=1,freqUpdate=50,freqTest=10,freqVerbose=10):

  text = ["env: "+env   ,                                            
          "seed: 5",
          "featExtractor: !!python/name:__main__.NothingToDo ''",
          "nbEpisodes: "+str(nbEpisodes),
          "maxLengthTrain : "+str(maxLengthTrain),
          "maxLengthTest : "+str(maxLengthTest),
          "nbTest : "+str(nbTest),
          "freqUpdate : "+str(freqUpdate),
          "freqTest : "+str(freqTest),
          "freqVerbose : "+str(freqVerbose),
          "startSteps : "+str(startSteps),
          ]
  with open(configName,'w') as f:
    
    f.write('\n'.join(text))
  return configName


class SACAgent(nn.Module):
  def __init__(self,env,opt): 
    super(SACAgent,self).__init__()
    self.opt = opt
    self.env = env
    self.test = False
    
    self.featureExtractor = self.opt['featExtractor'](self.env)

    self.n_actions = self.env.action_space.shape[0]
    self.obs_size = self.featureExtractor.outSize

    self.low = torch.from_numpy(self.env.action_space.low).to(torch.float32).to(device)
    self.high = torch.from_numpy(self.env.action_space.high).to(torch.float32).to(device)
    

    self.ac = ActorCritic(self.obs_size, self.n_actions,self.low,self.high).to(device).to(torch.float32)

    self.ac_target = copy.deepcopy(self.ac)
    for p in self.ac_target.parameters(): #on freeze tout les parametres du target qu'on mettera à jour manuellement
        p.requires_grad = False

    #Hyperparametres 
    self.alpha = 0.2
    self.rho = 0.995
    self.gamma = 0.99
    self.actor_lr, self.critic_lr = 0.001, 0.001
    self.batch_size = 100

    self.buffer = Memory(int(1e6))
    
    #BACKWARD 

    self.actor_optimizer = torch.optim.Adam(self.ac.actor.parameters(), lr = self.actor_lr)
    self.critic_optimizer = torch.optim.Adam(itertools.chain(self.ac.critic1.parameters(),self.ac.critic2.parameters()), lr = self.critic_lr)

    self.critic_criterion = nn.SmoothL1Loss().to(device)
    



  def act(self,obs):
    return self.ac.act(obs).detach().cpu().numpy()

  def loss_critic(self,obs, actions, rewards, next_obs, dones): 
    Q1 = self.ac.critic1(obs,actions)
    Q2 = self.ac.critic2(obs,actions)

    with torch.no_grad():
      next_actions,logp = self.ac.actor(next_obs)

      Q1_target = self.ac_target.critic1(next_obs,next_actions)
      Q2_target = self.ac_target.critic2(next_obs,next_actions)
      Q_target = torch.min(Q1_target,Q2_target)

      

      target = rewards + self.gamma * (1-dones) * (Q_target - self.alpha*logp.squeeze(1)).unsqueeze(1)
      
    loss1 = self.critic_criterion(target.squeeze(1),Q1).mean()
    loss2 = self.critic_criterion(target.squeeze(1),Q2).mean()
    return loss1+loss2

  def loss_actor(self,obs):

    action_sample,logp = self.ac.actor(obs)
    Q1 = self.ac.critic1(obs,action_sample)
    Q2 = self.ac.critic2(obs,action_sample)

    Q = torch.min(Q1,Q2)
    loss = -(Q-self.alpha*logp).mean()
    return loss

  def learn(self):
    if not self.test : 
      obs, actions, rewards, next_obs, dones = self.sample()
      
      #UPDATE DU CRITIC
      self.critic_optimizer.zero_grad()
      loss_critic = self.loss_critic(obs, actions, rewards, next_obs, dones)
      loss_critic.backward()
      self.critic_optimizer.step()
      
      for p1,p2 in zip(self.ac.critic1.parameters(),self.ac.critic2.parameters()): #On freeze les parametres du critic car on l'a deja optimiser
              p1.requires_grad = False
              p2.requires_grad = False
      #UPDATE DE ACTOR    
      self.actor_optimizer.zero_grad()
      loss_actor = self.loss_actor(obs)
      loss_actor.backward()
      self.actor_optimizer.step()

      for p1,p2 in zip(self.ac.critic1.parameters(),self.ac.critic2.parameters()): #On freeze les parametres du critic car on l'a deja optimiser
              p1.requires_grad = True
              p2.requires_grad = True
        
      self.update_targets()
    else:
      pass

  def store(self,ob, action, reward, next_ob, done):
    self.buffer.store((ob, action, reward, next_ob, done))

  def sample(self):
    sampled_trs = self.buffer.sample(self.batch_size)[2]
    obs, actions, rewards, next_obs, dones = [x for x in zip(*sampled_trs)]
    return (torch.vstack(obs).to(device), 
            torch.vstack(actions).to(device), 
            torch.tensor(rewards,device=device,dtype=torch.float32).reshape(len(rewards),1),
            torch.vstack(next_obs).to(device), 
            torch.tensor(dones,dtype=torch.float32,device=device).reshape(len(dones),1))


  @torch.no_grad()
  def update_targets(self):
    for p_ac, p_ac_target in zip(self.ac.parameters(),self.ac_target.parameters()):
      p_ac_target.data.mul_(self.rho) #on a fait les parametre target p_target <- rho*p_target 
      p_ac_target.data.add_((1-self.rho)*p_ac.data)

def main(config_file,name,agentClass):
  env, config, outdir, logger = init(config_file,name)

  freqTest = config["freqTest"]
  freqUpdate = config["freqUpdate"] 
  nbTest = config["nbTest"]
  env.seed(config["seed"])
  np.random.seed(config["seed"])
  episode_count = config["nbEpisodes"]
  maxLengthTrain = config["maxLengthTrain"]
  maxLengthTest = config["maxLengthTest"]
  startSteps = config["startSteps"]


  agent = agentClass(env,config)

  rsum = 0 #somme cumulé pour chaque épisode 
  mean = 0
  verbose = True
  itest = 0
  i=0
  reward = 0
  done = False
  condition_update = False


  for episode in range(episode_count):
    checkConfUpdate(outdir,config)

    rsum = 0 #on remet à 0 la somme cumulé à chaque episode
    ob = env.reset() #premiere observation, on commence le jeu 

    if episode % int(config["freqVerbose"]) == 0: #on affiche le jeu tout les 10 épisodes
      verbose = True
    else:
      verbose = False

    if episode%freqTest==0 and episode>=freqTest: #On test l'agent tout les 10 épisodes (2condition car episode=0 au debut)
      print("Test time!\n")
      mean = 0 #moyenne des rsum en phase de test pour voir comment notre agent se débrouille en moyenne
      agent.test = True #on dit à l'agent de passer en test, donc on ne stock plus les transitions et on ne s'entraine plus 

    if episode % freqTest == nbTest and episode > freqTest: #Fin du test
            print("End of test, mean reward=", mean / nbTest)
            itest += 1
            logger.direct_write("rewardTest", mean / nbTest, itest)
            agent.test = False
    

    
    j = 0 #nb de transition
    if verbose:
      #env.render()
      pass
    
    new_ob = agent.featureExtractor.getFeatures(ob) #on extrait les features de l'observation initial
    new_ob = torch.from_numpy(new_ob).to(device).to(torch.float32)
    while(True):
      if verbose: #on affiche à chaque étape le jeu
        #env.render()
        pass
        
      ob = new_ob

      if(i>=startSteps):
        action = agent.act(ob)
      else:
        action = env.action_space.sample()

      new_ob, reward, done, _ = env.step(action)

      new_ob = agent.featureExtractor.getFeatures(new_ob)
      new_ob = torch.from_numpy(new_ob).to(device).to(torch.float32)

      j+=1
      i+=1

      #on regarde si on a atteint la taille max de transitions 1er condition en phase de train, 2eme en phase de test 
      if ((maxLengthTrain > 0) and (not agent.test) and (j == maxLengthTrain)) or ( (agent.test) and (maxLengthTest > 0) and (j == maxLengthTest)):
                done = True #on force la fin de l'episode même si on n'a pas atteint un état final 
                print("forced done!")

      agent.store(ob, torch.from_numpy(action), reward, new_ob, done) #on sauvegarde nos transitions uniquement en phase de train , on ajoute le log_probs 
      rsum += reward

      
      
      if i > startSteps and i % freqUpdate == 0:
        for _ in range(freqUpdate):
            agent.learn()
    
      
      
      if(done): #si on atteint un etat final ou qu'on met fin a l'épisode
        print(str(episode) + " rsum=" + str(rsum) + ", " + str(j) + " actions ")
        logger.direct_write("reward", rsum, episode)
        mean += rsum #on sauvegarde le reward cumulé dans une variable qui nous servira à calculer la moyenne
        rsum = 0 #on remet le reward cumulé à 0 pour reprendre un nouveau épisode
        
        
        break
  
  env.close()
  return outdir

def run_env(nbEpisodes=1000):
  outdirs = []
  list_class = [SACAgent]
  list_name = ["SACAgent"]
  env_name = ['Pendulum-v0',
              "MountainCarContinuous-v0",
              #"LunarLanderContinuous-v2"
              ] #
  for agent_class,agent_name in zip(list_class,list_name):
    
    for env in env_name:
      print("######CHANGEMENT######\n")
      print(agent_name+"_"+env)
      file_config = make_config(agent_name+"_"+env+".yaml",env,nbEpisodes)
      outdir = main(file_config,agent_name+"_"+env,agent_class) 
      outdirs.append(outdir)
  return outdirs

if __name__ == "__main__": #On va mtn faire en sorte que notre algo s'entraine 
    outdirs = run_env(100)
    outdirs

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir=./XP/Pendulum-v0/SACAgent_Pendulum-v0_19-12-2021-15H40-45S

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir=./XP/MountainCarContinuous-v0/SACAgent_MountainCarContinuous-v0_19-12-2021-15H44-50S

